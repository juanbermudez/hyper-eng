# Verification Block
#
# Reusable verification block for prose workflows.
# Performs comprehensive QA including:
# - Automated checks (lint, typecheck, test, build)
# - UI verification via Tauri MCP
# - Sentry observability logging
# - Prose state validation
#
# Usage:
#   do verify-implementation(entity_id, expected_status)

# Agent for Tauri UI verification
agent ui-verifier:
  model: sonnet
  prompt: """You verify the Hypercraft UI using Tauri MCP tools.

Your job:
1. Connect to the running Hypercraft app
2. Find the specified project/task in the UI
3. Verify the status matches expected
4. Check for console errors
5. Take screenshots for evidence

Use these MCP tools:
- tauri_driver_session: Connect to app
- tauri_webview_screenshot: Capture state
- tauri_webview_find_element: Find DOM elements
- tauri_webview_execute_js: Run verification scripts
- tauri_read_logs: Check console

Return a structured verification report."""

# Agent for Sentry logging
agent sentry-logger:
  model: haiku
  prompt: """You log workflow events to Sentry for observability.

Use sentry-cli to send events:
```bash
sentry-cli send-event --message "..." --level info --tag key:value
```

Log: phase transitions, status changes, verification results, errors."""

# Agent for prose state validation
agent state-validator:
  model: sonnet
  prompt: """You validate the prose framework state is correct.

Check:
1. .prose/runs/{id}/state.md exists and shows correct position
2. Bindings are created in .prose/runs/{id}/bindings/
3. Agent memory is updated in .prose/agents/{name}/memory.md
4. MDX frontmatter matches expected status

Read files and report discrepancies."""

# Main verification block
block verify-implementation(entity_id, entity_type, expected_status, run_id):
  # Step 1: Log verification start to Sentry
  session: sentry-logger
    prompt: """Log verification phase start.

    ```bash
    sentry-cli send-event \
      --message "QA started: {entity_id}" \
      --level info \
      --tag workflow:hyper-workflow \
      --tag run_id:{run_id} \
      --tag phase:qa \
      --tag entity_id:{entity_id} \
      --tag expected_status:{expected_status}
    ```"""

  # Step 2: Run automated checks (parallel)
  parallel (on-fail: "collect"):
    lint_result = session "Run lint"
      prompt: """Run linting and report results.
      ```bash
      cd $(git rev-parse --show-toplevel)
      pnpm lint 2>&1 || echo "LINT_FAILED"
      ```
      Return: PASS or FAIL with details."""

    typecheck_result = session "Run typecheck"
      prompt: """Run TypeScript check and report results.
      ```bash
      cd $(git rev-parse --show-toplevel)
      pnpm typecheck 2>&1 || echo "TYPECHECK_FAILED"
      ```
      Return: PASS or FAIL with details."""

    test_result = session "Run tests"
      prompt: """Run test suite and report results.
      ```bash
      cd $(git rev-parse --show-toplevel)
      pnpm test 2>&1 || echo "TEST_FAILED"
      ```
      Return: PASS or FAIL with details."""

    build_result = session "Run build"
      prompt: """Run build and report results.
      ```bash
      cd $(git rev-parse --show-toplevel)
      pnpm build 2>&1 || echo "BUILD_FAILED"
      ```
      Return: PASS or FAIL with details."""

  # Step 3: Validate prose state
  let state_validation = session: state-validator
    prompt: """Validate prose framework state for run {run_id}.

    Check:
    1. Read $HYPER_WORKSPACE_ROOT/.prose/runs/{run_id}/state.md
       - Verify it exists
       - Check execution position markers

    2. List $HYPER_WORKSPACE_ROOT/.prose/runs/{run_id}/bindings/
       - Verify expected bindings exist

    3. Read $HYPER_WORKSPACE_ROOT/.prose/agents/hyper-captain/memory.md
       - Verify recent segment recorded

    4. Read MDX frontmatter:
       - If entity_type is 'project': $HYPER_WORKSPACE_ROOT/projects/{slug}/_project.mdx
       - If entity_type is 'task': $HYPER_WORKSPACE_ROOT/projects/{slug}/tasks/{task_file}.mdx
       - Verify status field matches {expected_status}

    Report all findings."""
    context: { entity_id, entity_type, expected_status, run_id }

  # Step 4: Verify UI updated (Tauri)
  let ui_verification = session: ui-verifier
    prompt: """Verify Hypercraft UI shows correct state.

    1. Connect to Hypercraft:
       Use tauri_driver_session action="start" port=9223

    2. Take baseline screenshot:
       Use tauri_webview_screenshot

    3. Find the entity in UI:
       If project: tauri_webview_find_element selector="[data-project-id='{entity_id}']"
       If task: tauri_webview_find_element selector="[data-task-id='{entity_id}']"

    4. Verify status display:
       Use tauri_webview_execute_js to check:
       - Element exists
       - Status badge/text shows '{expected_status}'
       - No error states visible

    5. Check console for errors:
       Use tauri_read_logs source="console" filter="error|Error"

    6. Take final screenshot

    Report: PASS or FAIL with evidence."""
    context: { entity_id, entity_type, expected_status }

  # Step 5: Aggregate results
  let verification_summary = session "Aggregate verification results"
    prompt: """Aggregate all verification results into a report.

    Automated Checks:
    - Lint: {lint_result}
    - Typecheck: {typecheck_result}
    - Test: {test_result}
    - Build: {build_result}

    Prose State: {state_validation}

    UI Verification: {ui_verification}

    Determine overall PASS or FAIL:
    - PASS: All automated checks pass AND state valid AND UI verified
    - FAIL: Any check failed

    If FAIL, identify which checks failed and why."""
    context: { lint_result, typecheck_result, test_result, build_result, state_validation, ui_verification }

  # Step 6: Log results to Sentry
  session: sentry-logger
    prompt: """Log verification results to Sentry.

    ```bash
    sentry-cli send-event \
      --message "QA {PASS_OR_FAIL}: {entity_id}" \
      --level {info_or_error} \
      --tag workflow:hyper-workflow \
      --tag run_id:{run_id} \
      --tag phase:qa \
      --tag entity_id:{entity_id} \
      --tag result:{PASS_OR_FAIL} \
      --extra lint:"{lint_result}" \
      --extra typecheck:"{typecheck_result}" \
      --extra test:"{test_result}" \
      --extra build:"{build_result}" \
      --extra ui_verified:"{ui_pass_or_fail}"
    ```"""
    context: verification_summary

  # Step 7: Return result
  if **verification passed**:
    output verification_result = verification_summary
  else:
    # Return failure with details for retry logic
    output verification_result = session "Format failure report"
      prompt: """Format a failure report showing:
      - Which checks failed
      - Error details
      - Suggested fixes
      - Next steps"""
      context: verification_summary


# Verification loop block - retries until pass or max attempts
block verify-with-retry(entity_id, entity_type, expected_status, run_id, max_attempts):
  let attempt = 1
  let verification_passed = false

  loop until **verification_passed** (max: {max_attempts}):
    # Log attempt
    session: sentry-logger
      prompt: """Log verification attempt {attempt} of {max_attempts}."""

    # Run verification
    do verify-implementation(entity_id, entity_type, expected_status, run_id)

    if **verification passed**:
      verification_passed = true
    else:
      # If failed, spawn fix session
      session "Attempt to fix failures"
        prompt: """Review verification failures and attempt fixes.

        Based on the failure report:
        1. Identify the root cause
        2. Make necessary code changes
        3. Re-run the specific failing check to verify fix

        Do NOT proceed if you cannot fix the issue - report it instead."""
        context: verification_result

      attempt = attempt + 1

  if **not verification_passed after max attempts**:
    # Update status back to in-progress
    session "Revert status on verification failure"
      prompt: """Verification failed after {max_attempts} attempts.

      Update status back to in-progress:
      ```bash
      ${CLAUDE_PLUGIN_ROOT}/binaries/hyper {entity_type} update "{entity_id}" --status "in-progress"
      ```

      Log failure to Sentry and report to user."""
